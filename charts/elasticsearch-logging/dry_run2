NAME: es
LAST DEPLOYED: Fri Nov 20 14:48:03 2020
NAMESPACE: default
STATUS: pending-install
REVISION: 1
HOOKS:
---
# Source: elasticsearch-logging-wiremind_/charts/es-data-cold/templates/test/test-elasticsearch-health.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "es-szkwb-test"
  annotations:
    "helm.sh/hook": test-success
spec:
  securityContext:
    fsGroup: 1000
    runAsUser: 1000
  containers:
  - name: "es-uvsnv-test"
    image: "docker.elastic.co/elasticsearch/elasticsearch:7.9.3"
    imagePullPolicy: "IfNotPresent"
    command:
      - "sh"
      - "-c"
      - |
        #!/usr/bin/env bash -e
        curl -XGET --fail 'logging-data-cold:9200/_cluster/health?wait_for_status=green&timeout=1s'
  restartPolicy: Never
---
# Source: elasticsearch-logging-wiremind_/charts/es-data-hot/templates/test/test-elasticsearch-health.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "es-yftcq-test"
  annotations:
    "helm.sh/hook": test-success
spec:
  securityContext:
    fsGroup: 1000
    runAsUser: 1000
  containers:
  - name: "es-rgarz-test"
    image: "docker.elastic.co/elasticsearch/elasticsearch:7.9.3"
    imagePullPolicy: "IfNotPresent"
    command:
      - "sh"
      - "-c"
      - |
        #!/usr/bin/env bash -e
        curl -XGET --fail 'logging-data-hot:9200/_cluster/health?wait_for_status=green&timeout=1s'
  restartPolicy: Never
---
# Source: elasticsearch-logging-wiremind_/charts/es-data-warm/templates/test/test-elasticsearch-health.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "es-bsgcq-test"
  annotations:
    "helm.sh/hook": test-success
spec:
  securityContext:
    fsGroup: 1000
    runAsUser: 1000
  containers:
  - name: "es-twovc-test"
    image: "docker.elastic.co/elasticsearch/elasticsearch:7.9.3"
    imagePullPolicy: "IfNotPresent"
    command:
      - "sh"
      - "-c"
      - |
        #!/usr/bin/env bash -e
        curl -XGET --fail 'logging-data-warm:9200/_cluster/health?wait_for_status=green&timeout=1s'
  restartPolicy: Never
---
# Source: elasticsearch-logging-wiremind_/charts/es-master/templates/test/test-elasticsearch-health.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "es-gvoiy-test"
  annotations:
    "helm.sh/hook": test-success
spec:
  securityContext:
    fsGroup: 1000
    runAsUser: 1000
  containers:
  - name: "es-egzfr-test"
    image: "docker.elastic.co/elasticsearch/elasticsearch:7.9.3"
    imagePullPolicy: "IfNotPresent"
    command:
      - "sh"
      - "-c"
      - |
        #!/usr/bin/env bash -e
        curl -XGET --fail 'logging-master:9200/_cluster/health?wait_for_status=green&timeout=1s'
  restartPolicy: Never
MANIFEST:
---
# Source: elasticsearch-logging-wiremind_/charts/es-data-cold/templates/poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: "logging-data-cold-pdb"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: "logging-data-cold"
---
# Source: elasticsearch-logging-wiremind_/charts/es-data-hot/templates/poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: "logging-data-hot-pdb"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: "logging-data-hot"
---
# Source: elasticsearch-logging-wiremind_/charts/es-data-warm/templates/poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: "logging-data-warm-pdb"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: "logging-data-warm"
---
# Source: elasticsearch-logging-wiremind_/charts/es-master/templates/poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: "logging-master-pdb"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: "logging-master"
---
# Source: elasticsearch-logging-wiremind_/charts/es-data-cold/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: logging-data-cold-config
  labels:
    heritage: "Helm"
    release: "es"
    chart: "es-data-cold"
    app: "logging-data-cold"
data:
  elasticsearch.yml: |
    node:
      attr:
        data: cold
    xpack:
      security:
        enabled: false
        transport:
          ssl:
            enabled: false
        http:
          ssl:
            enabled: false
---
# Source: elasticsearch-logging-wiremind_/charts/es-data-hot/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: logging-data-hot-config
  labels:
    heritage: "Helm"
    release: "es"
    chart: "es-data-hot"
    app: "logging-data-hot"
data:
  elasticsearch.yml: |
    node:
      attr:
        data: hot
    thread_pool:
      search:
        size: 60
        min_queue_size: 1000
        max_queue_size: 10000
    xpack:
      security:
        enabled: false
        transport:
          ssl:
            enabled: false
        http:
          ssl:
            enabled: false
---
# Source: elasticsearch-logging-wiremind_/charts/es-data-warm/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: logging-data-warm-config
  labels:
    heritage: "Helm"
    release: "es"
    chart: "es-data-warm"
    app: "logging-data-warm"
data:
  elasticsearch.yml: |
    node:
      attr:
        data: warm
    xpack:
      security:
        enabled: false
        transport:
          ssl:
            enabled: false
        http:
          ssl:
            enabled: false
---
# Source: elasticsearch-logging-wiremind_/charts/es-master/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: logging-master-config
  labels:
    heritage: "Helm"
    release: "es"
    chart: "es-master"
    app: "logging-master"
data:
  elasticsearch.yml: |
    xpack:
      security:
        enabled: false
        transport:
          ssl:
            enabled: false
        http:
          ssl:
            enabled: false
---
# Source: elasticsearch-logging-wiremind_/charts/kibana/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: es-kibana-config
  labels: 
    app: kibana
    release: "es"
    heritage: Helm
data:
  kibana.yml: |
    server.name: kibana
    server.host: "0"
    elasticsearch.requestTimeout: 180000
---
# Source: elasticsearch-logging-wiremind_/charts/es-data-cold/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: logging-data-cold
  labels:
    heritage: "Helm"
    release: "es"
    chart: "es-data-cold"
    app: "logging-data-cold"
  annotations:
    {}
spec:
  type: ClusterIP
  selector:
    release: "es"
    chart: "es-data-cold"
    app: "logging-data-cold"
  ports:
  - name: http
    protocol: TCP
    port: 9200
  - name: transport
    protocol: TCP
    port: 9300
---
# Source: elasticsearch-logging-wiremind_/charts/es-data-cold/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: logging-data-cold-headless
  labels:
    heritage: "Helm"
    release: "es"
    chart: "es-data-cold"
    app: "logging-data-cold"
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  clusterIP: None # This is needed for statefulset hostnames like elasticsearch-0 to resolve
  # Create endpoints also if the related pod isn't ready
  publishNotReadyAddresses: true
  selector:
    app: "logging-data-cold"
  ports:
  - name: http
    port: 9200
  - name: transport
    port: 9300
---
# Source: elasticsearch-logging-wiremind_/charts/es-data-hot/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: logging-data-hot
  labels:
    heritage: "Helm"
    release: "es"
    chart: "es-data-hot"
    app: "logging-data-hot"
  annotations:
    {}
spec:
  type: ClusterIP
  selector:
    release: "es"
    chart: "es-data-hot"
    app: "logging-data-hot"
  ports:
  - name: http
    protocol: TCP
    port: 9200
  - name: transport
    protocol: TCP
    port: 9300
---
# Source: elasticsearch-logging-wiremind_/charts/es-data-hot/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: logging-data-hot-headless
  labels:
    heritage: "Helm"
    release: "es"
    chart: "es-data-hot"
    app: "logging-data-hot"
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  clusterIP: None # This is needed for statefulset hostnames like elasticsearch-0 to resolve
  # Create endpoints also if the related pod isn't ready
  publishNotReadyAddresses: true
  selector:
    app: "logging-data-hot"
  ports:
  - name: http
    port: 9200
  - name: transport
    port: 9300
---
# Source: elasticsearch-logging-wiremind_/charts/es-data-warm/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: logging-data-warm
  labels:
    heritage: "Helm"
    release: "es"
    chart: "es-data-warm"
    app: "logging-data-warm"
  annotations:
    {}
spec:
  type: ClusterIP
  selector:
    release: "es"
    chart: "es-data-warm"
    app: "logging-data-warm"
  ports:
  - name: http
    protocol: TCP
    port: 9200
  - name: transport
    protocol: TCP
    port: 9300
---
# Source: elasticsearch-logging-wiremind_/charts/es-data-warm/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: logging-data-warm-headless
  labels:
    heritage: "Helm"
    release: "es"
    chart: "es-data-warm"
    app: "logging-data-warm"
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  clusterIP: None # This is needed for statefulset hostnames like elasticsearch-0 to resolve
  # Create endpoints also if the related pod isn't ready
  publishNotReadyAddresses: true
  selector:
    app: "logging-data-warm"
  ports:
  - name: http
    port: 9200
  - name: transport
    port: 9300
---
# Source: elasticsearch-logging-wiremind_/charts/es-master/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: logging-master
  labels:
    heritage: "Helm"
    release: "es"
    chart: "es-master"
    app: "logging-master"
  annotations:
    {}
spec:
  type: ClusterIP
  selector:
    release: "es"
    chart: "es-master"
    app: "logging-master"
  ports:
  - name: http
    protocol: TCP
    port: 9200
  - name: transport
    protocol: TCP
    port: 9300
---
# Source: elasticsearch-logging-wiremind_/charts/es-master/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: logging-master-headless
  labels:
    heritage: "Helm"
    release: "es"
    chart: "es-master"
    app: "logging-master"
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  clusterIP: None # This is needed for statefulset hostnames like elasticsearch-0 to resolve
  # Create endpoints also if the related pod isn't ready
  publishNotReadyAddresses: true
  selector:
    app: "logging-master"
  ports:
  - name: http
    port: 9200
  - name: transport
    port: 9300
---
# Source: elasticsearch-logging-wiremind_/charts/kibana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: es-kibana
  labels: 
    app: kibana
    release: "es"
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - port: 5601
      protocol: TCP
      name: http
      targetPort: 5601
  selector:
    app: kibana
    release: "es"
---
# Source: elasticsearch-logging-wiremind_/charts/prometheus-elasticsearch-exporter/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: es-prometheus-elasticsearch-exporter
  labels:
    chart: prometheus-elasticsearch-exporter-4.0.0
    app: prometheus-elasticsearch-exporter
    release: "es"
    heritage: "Helm"
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 9108
      protocol: TCP
  selector:
    app: prometheus-elasticsearch-exporter
    release: "es"
---
# Source: elasticsearch-logging-wiremind_/charts/kibana/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: es-kibana
  labels: 
    app: kibana
    release: "es"
    heritage: Helm
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: kibana
      release: "es"
  template:
    metadata:
      labels:
        app: kibana
        release: "es"
      annotations:
        
        configchecksum: b263ac2d91fff261031d8fd14f2f2d8af3e8c8df9869bfc6c9dbd59eae67a12
    spec:
      securityContext:
        fsGroup: 1000
      volumes:
        - name: kibanaconfig
          configMap:
            name: es-kibana-config
      containers:
      - name: kibana
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "docker.elastic.co/kibana/kibana:7.9.3"
        imagePullPolicy: "IfNotPresent"
        env:
          - name: ELASTICSEARCH_HOSTS
            value: "http://logging-data-hot:9200"
          - name: SERVER_HOST
            value: "0.0.0.0"
          - name: NODE_OPTIONS
            value: --max-old-space-size=1800
        readinessProbe:
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
          exec:
            command:
              - sh
              - -c
              - |
                #!/usr/bin/env bash -e

                # Disable nss cache to avoid filling dentry cache when calling curl
                # This is required with Kibana Docker using nss < 3.52
                export NSS_SDB_USE_CACHE=no

                http () {
                    local path="${1}"
                    set -- -XGET -s --fail -L

                    if [ -n "${ELASTICSEARCH_USERNAME}" ] && [ -n "${ELASTICSEARCH_PASSWORD}" ]; then
                      set -- "$@" -u "${ELASTICSEARCH_USERNAME}:${ELASTICSEARCH_PASSWORD}"
                    fi

                    STATUS=$(curl --output /dev/null --write-out "%{http_code}" -k "$@" "http://localhost:5601${path}")
                    if [[ "${STATUS}" -eq 200 ]]; then
                      exit 0
                    fi

                    echo "Error: Got HTTP code ${STATUS} but expected a 200"
                    exit 1
                }

                http "/app/kibana"
        ports:
        - containerPort: 5601
        resources:
          limits:
            cpu: 2
            memory: 2Gi
          requests:
            cpu: 1000m
            memory: 2Gi
        volumeMounts:
          - name: kibanaconfig
            mountPath: /usr/share/kibana/config/kibana.yml
            subPath: kibana.yml
---
# Source: elasticsearch-logging-wiremind_/charts/prometheus-elasticsearch-exporter/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: es-prometheus-elasticsearch-exporter
  labels:
    chart: prometheus-elasticsearch-exporter-4.0.0
    app: prometheus-elasticsearch-exporter
    release: "es"
    heritage: "Helm"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus-elasticsearch-exporter
      release: "es"
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: prometheus-elasticsearch-exporter
        release: "es"
    spec:
      serviceAccountName: default
      restartPolicy: Always
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
      containers:
        - name: exporter
          env:
          image: "justwatch/elasticsearch_exporter:1.1.0"
          imagePullPolicy: IfNotPresent
          command: ["elasticsearch_exporter",
                    "--log.format=logfmt",
                    "--log.level=info",
                    "--es.uri=http://localhost:9200",
                    "--es.all",
                    "--es.indices",
                    "--es.indices_settings",
                    "--es.shards",
                    "--es.snapshots",
                    "--es.timeout=30s",
                    "--web.listen-address=:9108",
                    "--web.telemetry-path=/metrics"]
          securityContext:
            capabilities:
              drop:
                - SETPCAP
                - MKNOD
                - AUDIT_WRITE
                - CHOWN
                - NET_RAW
                - DAC_OVERRIDE
                - FOWNER
                - FSETID
                - KILL
                - SETGID
                - SETUID
                - NET_BIND_SERVICE
                - SYS_CHROOT
                - SETFCAP
            readOnlyRootFilesystem: true
          resources:
            {}
          ports:
            - containerPort: 9108
              name: http
          livenessProbe:
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 5
            timeoutSeconds: 5
            periodSeconds: 5
          readinessProbe:
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 1
            timeoutSeconds: 5
            periodSeconds: 5
          lifecycle:
            preStop:
              exec:
                command: ["/bin/bash", "-c", "sleep 20"]
          volumeMounts:
      volumes:
---
# Source: elasticsearch-logging-wiremind_/charts/es-data-cold/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: logging-data-cold
  labels:
    heritage: "Helm"
    release: "es"
    chart: "es-data-cold"
    app: "logging-data-cold"
  annotations:
    esMajorVersion: "7"
spec:
  serviceName: logging-data-cold-headless
  selector:
    matchLabels:
      app: "logging-data-cold"
  replicas: 1
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: logging-data-cold
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 100M
  template:
    metadata:
      name: "logging-data-cold"
      labels:
        heritage: "Helm"
        release: "es"
        chart: "es-data-cold"
        app: "logging-data-cold"
      annotations:
        
        configchecksum: cab94ead94c3709b6ff742c3652c34f6058f43c69448bfc570d75908eff3575
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - "logging-data-cold"
      terminationGracePeriodSeconds: 120
      volumes:
        - name: esconfig
          configMap:
            name: logging-data-cold-config
      enableServiceLinks: true
      initContainers:
      - name: configure-sysctl
        securityContext:
          runAsUser: 0
          privileged: true
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.9.3"
        imagePullPolicy: "IfNotPresent"
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        resources:
          {}

      containers:
      - name: "es-data-cold"
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.9.3"
        imagePullPolicy: "IfNotPresent"
        readinessProbe:
          exec:
            command:
              - sh
              - -c
              - |
                #!/usr/bin/env bash -e
                # If the node is starting up wait for the cluster to be ready (request params: "wait_for_status=green&timeout=1s" )
                # Once it has started only check that the node itself is responding
                START_FILE=/tmp/.es_start_file

                # Disable nss cache to avoid filling dentry cache when calling curl
                # This is required with Elasticsearch Docker using nss < 3.52
                export NSS_SDB_USE_CACHE=no

                http () {
                  local path="${1}"
                  local args="${2}"
                  set -- -XGET -s

                  if [ "$args" != "" ]; then
                    set -- "$@" $args
                  fi

                  if [ -n "${ELASTIC_USERNAME}" ] && [ -n "${ELASTIC_PASSWORD}" ]; then
                    set -- "$@" -u "${ELASTIC_USERNAME}:${ELASTIC_PASSWORD}"
                  fi

                  curl --output /dev/null -k "$@" "http://127.0.0.1:9200${path}"
                }

                if [ -f "${START_FILE}" ]; then
                  echo 'Elasticsearch is already running, lets check the node is healthy'
                  HTTP_CODE=$(http "/" "-w %{http_code}")
                  RC=$?
                  if [[ ${RC} -ne 0 ]]; then
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with RC ${RC}"
                    exit ${RC}
                  fi
                  # ready if HTTP code 200, 503 is tolerable if ES version is 6.x
                  if [[ ${HTTP_CODE} == "200" ]]; then
                    exit 0
                  elif [[ ${HTTP_CODE} == "503" && "7" == "6" ]]; then
                    exit 0
                  else
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with HTTP code ${HTTP_CODE}"
                    exit 1
                  fi

                else
                  echo 'Waiting for elasticsearch cluster to become ready (request params: "wait_for_status=green&timeout=1s" )'
                  if http "/_cluster/health?wait_for_status=green&timeout=1s" "--fail" ; then
                    touch ${START_FILE}
                    exit 0
                  else
                    echo 'Cluster is not yet ready (request params: "wait_for_status=green&timeout=1s" )'
                    exit 1
                  fi
                fi
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 3
          timeoutSeconds: 5
        ports:
        - name: http
          containerPort: 9200
        - name: transport
          containerPort: 9300
        resources:
          limits:
            cpu: 6
            memory: 16Gi
          requests:
            cpu: 10m
            memory: 128M
        env:
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: discovery.seed_hosts
            value: "logging-master-headless"
          - name: cluster.name
            value: "logging"
          - name: network.host
            value: "0.0.0.0"
          - name: ES_JAVA_OPTS
            value: "-Xmx128m -Xms128m"
          - name: node.data
            value: "true"
          - name: node.ingest
            value: "false"
          - name: node.master
            value: "false"
        volumeMounts:
          - name: "logging-data-cold"
            mountPath: /usr/share/elasticsearch/data

          - name: esconfig
            mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
            subPath: elasticsearch.yml
---
# Source: elasticsearch-logging-wiremind_/charts/es-data-hot/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: logging-data-hot
  labels:
    heritage: "Helm"
    release: "es"
    chart: "es-data-hot"
    app: "logging-data-hot"
  annotations:
    esMajorVersion: "7"
spec:
  serviceName: logging-data-hot-headless
  selector:
    matchLabels:
      app: "logging-data-hot"
  replicas: 1
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: logging-data-hot
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 100M
  template:
    metadata:
      name: "logging-data-hot"
      labels:
        heritage: "Helm"
        release: "es"
        chart: "es-data-hot"
        app: "logging-data-hot"
      annotations:
        
        configchecksum: 8bba2d1a93b2e72114ed043c15339ab39ec57cf278a2f322bc2cea52e5ef1c0
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - "logging-data-hot"
      terminationGracePeriodSeconds: 120
      volumes:
        - name: esconfig
          configMap:
            name: logging-data-hot-config
      enableServiceLinks: true
      initContainers:
      - name: configure-sysctl
        securityContext:
          runAsUser: 0
          privileged: true
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.9.3"
        imagePullPolicy: "IfNotPresent"
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        resources:
          {}

      containers:
      - name: "es-data-hot"
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.9.3"
        imagePullPolicy: "IfNotPresent"
        readinessProbe:
          exec:
            command:
              - sh
              - -c
              - |
                #!/usr/bin/env bash -e
                # If the node is starting up wait for the cluster to be ready (request params: "wait_for_status=green&timeout=1s" )
                # Once it has started only check that the node itself is responding
                START_FILE=/tmp/.es_start_file

                # Disable nss cache to avoid filling dentry cache when calling curl
                # This is required with Elasticsearch Docker using nss < 3.52
                export NSS_SDB_USE_CACHE=no

                http () {
                  local path="${1}"
                  local args="${2}"
                  set -- -XGET -s

                  if [ "$args" != "" ]; then
                    set -- "$@" $args
                  fi

                  if [ -n "${ELASTIC_USERNAME}" ] && [ -n "${ELASTIC_PASSWORD}" ]; then
                    set -- "$@" -u "${ELASTIC_USERNAME}:${ELASTIC_PASSWORD}"
                  fi

                  curl --output /dev/null -k "$@" "http://127.0.0.1:9200${path}"
                }

                if [ -f "${START_FILE}" ]; then
                  echo 'Elasticsearch is already running, lets check the node is healthy'
                  HTTP_CODE=$(http "/" "-w %{http_code}")
                  RC=$?
                  if [[ ${RC} -ne 0 ]]; then
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with RC ${RC}"
                    exit ${RC}
                  fi
                  # ready if HTTP code 200, 503 is tolerable if ES version is 6.x
                  if [[ ${HTTP_CODE} == "200" ]]; then
                    exit 0
                  elif [[ ${HTTP_CODE} == "503" && "7" == "6" ]]; then
                    exit 0
                  else
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with HTTP code ${HTTP_CODE}"
                    exit 1
                  fi

                else
                  echo 'Waiting for elasticsearch cluster to become ready (request params: "wait_for_status=green&timeout=1s" )'
                  if http "/_cluster/health?wait_for_status=green&timeout=1s" "--fail" ; then
                    touch ${START_FILE}
                    exit 0
                  else
                    echo 'Cluster is not yet ready (request params: "wait_for_status=green&timeout=1s" )'
                    exit 1
                  fi
                fi
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 3
          timeoutSeconds: 5
        ports:
        - name: http
          containerPort: 9200
        - name: transport
          containerPort: 9300
        resources:
          limits:
            cpu: 15
            memory: 56Gi
          requests:
            cpu: 10m
            memory: 128M
        env:
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: discovery.seed_hosts
            value: "logging-master-headless"
          - name: cluster.name
            value: "logging"
          - name: network.host
            value: "0.0.0.0"
          - name: ES_JAVA_OPTS
            value: "-Xmx128m  -Xms128m"
          - name: node.data
            value: "true"
          - name: node.ingest
            value: "true"
          - name: node.master
            value: "false"
        volumeMounts:
          - name: "logging-data-hot"
            mountPath: /usr/share/elasticsearch/data

          - name: esconfig
            mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
            subPath: elasticsearch.yml
---
# Source: elasticsearch-logging-wiremind_/charts/es-data-warm/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: logging-data-warm
  labels:
    heritage: "Helm"
    release: "es"
    chart: "es-data-warm"
    app: "logging-data-warm"
  annotations:
    esMajorVersion: "7"
spec:
  serviceName: logging-data-warm-headless
  selector:
    matchLabels:
      app: "logging-data-warm"
  replicas: 1
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: logging-data-warm
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 100M
  template:
    metadata:
      name: "logging-data-warm"
      labels:
        heritage: "Helm"
        release: "es"
        chart: "es-data-warm"
        app: "logging-data-warm"
      annotations:
        
        configchecksum: 8fb96b66d6d6d98ff35eb588874693f3252821f101dd207440ecb3b485072a0
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - "logging-data-warm"
      terminationGracePeriodSeconds: 120
      volumes:
        - name: esconfig
          configMap:
            name: logging-data-warm-config
      enableServiceLinks: true
      initContainers:
      - name: configure-sysctl
        securityContext:
          runAsUser: 0
          privileged: true
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.9.3"
        imagePullPolicy: "IfNotPresent"
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        resources:
          {}

      containers:
      - name: "es-data-warm"
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.9.3"
        imagePullPolicy: "IfNotPresent"
        readinessProbe:
          exec:
            command:
              - sh
              - -c
              - |
                #!/usr/bin/env bash -e
                # If the node is starting up wait for the cluster to be ready (request params: "wait_for_status=green&timeout=1s" )
                # Once it has started only check that the node itself is responding
                START_FILE=/tmp/.es_start_file

                # Disable nss cache to avoid filling dentry cache when calling curl
                # This is required with Elasticsearch Docker using nss < 3.52
                export NSS_SDB_USE_CACHE=no

                http () {
                  local path="${1}"
                  local args="${2}"
                  set -- -XGET -s

                  if [ "$args" != "" ]; then
                    set -- "$@" $args
                  fi

                  if [ -n "${ELASTIC_USERNAME}" ] && [ -n "${ELASTIC_PASSWORD}" ]; then
                    set -- "$@" -u "${ELASTIC_USERNAME}:${ELASTIC_PASSWORD}"
                  fi

                  curl --output /dev/null -k "$@" "http://127.0.0.1:9200${path}"
                }

                if [ -f "${START_FILE}" ]; then
                  echo 'Elasticsearch is already running, lets check the node is healthy'
                  HTTP_CODE=$(http "/" "-w %{http_code}")
                  RC=$?
                  if [[ ${RC} -ne 0 ]]; then
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with RC ${RC}"
                    exit ${RC}
                  fi
                  # ready if HTTP code 200, 503 is tolerable if ES version is 6.x
                  if [[ ${HTTP_CODE} == "200" ]]; then
                    exit 0
                  elif [[ ${HTTP_CODE} == "503" && "7" == "6" ]]; then
                    exit 0
                  else
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with HTTP code ${HTTP_CODE}"
                    exit 1
                  fi

                else
                  echo 'Waiting for elasticsearch cluster to become ready (request params: "wait_for_status=green&timeout=1s" )'
                  if http "/_cluster/health?wait_for_status=green&timeout=1s" "--fail" ; then
                    touch ${START_FILE}
                    exit 0
                  else
                    echo 'Cluster is not yet ready (request params: "wait_for_status=green&timeout=1s" )'
                    exit 1
                  fi
                fi
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 3
          timeoutSeconds: 5
        ports:
        - name: http
          containerPort: 9200
        - name: transport
          containerPort: 9300
        resources:
          limits:
            cpu: 15
            memory: 18Gi
          requests:
            cpu: 10m
            memory: 128M
        env:
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: discovery.seed_hosts
            value: "logging-master-headless"
          - name: cluster.name
            value: "logging"
          - name: network.host
            value: "0.0.0.0"
          - name: ES_JAVA_OPTS
            value: "-Xmx128m -Xms128m"
          - name: node.data
            value: "true"
          - name: node.ingest
            value: "false"
          - name: node.master
            value: "false"
        volumeMounts:
          - name: "logging-data-warm"
            mountPath: /usr/share/elasticsearch/data

          - name: esconfig
            mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
            subPath: elasticsearch.yml
---
# Source: elasticsearch-logging-wiremind_/charts/es-master/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: logging-master
  labels:
    heritage: "Helm"
    release: "es"
    chart: "es-master"
    app: "logging-master"
  annotations:
    esMajorVersion: "7"
spec:
  serviceName: logging-master-headless
  selector:
    matchLabels:
      app: "logging-master"
  replicas: 3
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: logging-master
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 30Gi
  template:
    metadata:
      name: "logging-master"
      labels:
        heritage: "Helm"
        release: "es"
        chart: "es-master"
        app: "logging-master"
      annotations:
        
        configchecksum: 86dd2ec1ccb5b157d20bfd1b425746b66a4d23a8e8b5938b8ec03a8eebd7b3c
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - "logging-master"
      terminationGracePeriodSeconds: 120
      volumes:
        - name: esconfig
          configMap:
            name: logging-master-config
      enableServiceLinks: true
      initContainers:
      - name: configure-sysctl
        securityContext:
          runAsUser: 0
          privileged: true
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.9.3"
        imagePullPolicy: "IfNotPresent"
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        resources:
          {}

      containers:
      - name: "es-master"
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.9.3"
        imagePullPolicy: "IfNotPresent"
        readinessProbe:
          exec:
            command:
              - sh
              - -c
              - |
                #!/usr/bin/env bash -e
                # If the node is starting up wait for the cluster to be ready (request params: "wait_for_status=green&timeout=1s" )
                # Once it has started only check that the node itself is responding
                START_FILE=/tmp/.es_start_file

                # Disable nss cache to avoid filling dentry cache when calling curl
                # This is required with Elasticsearch Docker using nss < 3.52
                export NSS_SDB_USE_CACHE=no

                http () {
                  local path="${1}"
                  local args="${2}"
                  set -- -XGET -s

                  if [ "$args" != "" ]; then
                    set -- "$@" $args
                  fi

                  if [ -n "${ELASTIC_USERNAME}" ] && [ -n "${ELASTIC_PASSWORD}" ]; then
                    set -- "$@" -u "${ELASTIC_USERNAME}:${ELASTIC_PASSWORD}"
                  fi

                  curl --output /dev/null -k "$@" "http://127.0.0.1:9200${path}"
                }

                if [ -f "${START_FILE}" ]; then
                  echo 'Elasticsearch is already running, lets check the node is healthy'
                  HTTP_CODE=$(http "/" "-w %{http_code}")
                  RC=$?
                  if [[ ${RC} -ne 0 ]]; then
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with RC ${RC}"
                    exit ${RC}
                  fi
                  # ready if HTTP code 200, 503 is tolerable if ES version is 6.x
                  if [[ ${HTTP_CODE} == "200" ]]; then
                    exit 0
                  elif [[ ${HTTP_CODE} == "503" && "7" == "6" ]]; then
                    exit 0
                  else
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with HTTP code ${HTTP_CODE}"
                    exit 1
                  fi

                else
                  echo 'Waiting for elasticsearch cluster to become ready (request params: "wait_for_status=green&timeout=1s" )'
                  if http "/_cluster/health?wait_for_status=green&timeout=1s" "--fail" ; then
                    touch ${START_FILE}
                    exit 0
                  else
                    echo 'Cluster is not yet ready (request params: "wait_for_status=green&timeout=1s" )'
                    exit 1
                  fi
                fi
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 3
          timeoutSeconds: 5
        ports:
        - name: http
          containerPort: 9200
        - name: transport
          containerPort: 9300
        resources:
          limits:
            cpu: "2"
            memory: 1Gi
          requests:
            cpu: 10m
            memory: 128M
        env:
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: cluster.initial_master_nodes
            value: "logging-master-0,logging-master-1,logging-master-2,"
          - name: discovery.seed_hosts
            value: "logging-master-headless"
          - name: cluster.name
            value: "logging"
          - name: network.host
            value: "0.0.0.0"
          - name: ES_JAVA_OPTS
            value: "-Xms128m -Xmx128m"
          - name: node.data
            value: "false"
          - name: node.ingest
            value: "false"
          - name: node.master
            value: "true"
        volumeMounts:
          - name: "logging-master"
            mountPath: /usr/share/elasticsearch/data

          - name: esconfig
            mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
            subPath: elasticsearch.yml
---
# Source: elasticsearch-logging-wiremind_/charts/es-data-hot/templates/ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: logging-data-hot
  labels:
    app: es-data-hot
    release: es
    heritage: Helm
  annotations:
    kubernetes.io/tls-acme: "false"
    nginx.ingress.kubernetes.io/auth-realm: Authentication Required
    nginx.ingress.kubernetes.io/auth-secret: basic-auth
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/client-body-buffer-size: 9M
    nginx.ingress.kubernetes.io/proxy-body-size: 600m
spec:
  tls:
    - hosts:
        - elasticsearch.setme.org
      secretName: wildcard-setme.org-tls
  rules:
    - host: elasticsearch.setme.org
      http:
        paths:
          - path: /
            backend:
              serviceName: logging-data-hot
              servicePort: 9200
---
# Source: elasticsearch-logging-wiremind_/charts/kibana/templates/ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: es-kibana
  labels: 
    app: kibana
    release: "es"
    heritage: Helm
  annotations:
    nginx.ingress.kubernetes.io/auth-signin: https://sso.setme.org/oauth2/start
    nginx.ingress.kubernetes.io/auth-url: http://oauth2-proxy.dex.svc.cluster.local/oauth2/auth
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "190"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "190"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "190"
spec:
  tls:
    - hosts:
      - kibana.setme.org
      secretName: wildcard-setme.org-tls
  rules:
    - host: kibana.setme.org
      http:
        paths:
          - path: /
            backend:
              serviceName: es-kibana
              servicePort: 5601

